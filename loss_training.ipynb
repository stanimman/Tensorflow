{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from matplotlib.colors import ListedColormap #Uncomment these if you have installed matplotlib (pip3 install matplotlib)\n",
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"x_circles.data\",'rb') as f:\n",
    "    x = np.load(f)\n",
    "with open(\"y_circles.data\",'rb') as f:\n",
    "    y = np.load(f).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#scatter(x[:,0], x[:,1], c=y, cmap= ListedColormap(['#FF0000', '#0000FF']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, batchsize, xsize, optimizer=tf.train.GradientDescentOptimizer):\n",
    "        self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "        self.x = tf.placeholder(shape=(batchsize, xsize), dtype=tf.float32, name=\"x\")\n",
    "        self.y = tf.placeholder(shape=(batchsize,1), dtype=tf.float32, name=\"y\")\n",
    "        w = tf.get_variable(\"w\", shape=(xsize,1))\n",
    "        b = tf.get_variable(\"b\", shape=(1))\n",
    "        logit = tf.add(tf.matmul(self.x,w),b) #Broadcast\n",
    "        self.proba = tf.sigmoid(logit)\n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=self.y)) \n",
    "        self.train = optimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "initializer = tf.random_uniform_initializer(minval=-0.05, maxval=0.05)\n",
    "with tf.variable_scope(\"LR\", reuse=None, initializer=initializer):\n",
    "    lr = LogisticRegression(100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "[(t.name, t.get_shape().as_list()) for t in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer()) #Set w,b to random values from distribution\n",
    "sess.run(tf.assign(lr.learning_rate, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "output = sess.run([lr.loss, lr.train], feed_dict={lr.x:x, lr.y:y})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    loss, proba, _ = sess.run([lr.loss, lr.proba, lr.train], feed_dict={lr.x:x, lr.y:y})\n",
    "    if i % 1000 == 0:\n",
    "        proba = sess.run(lr.proba, feed_dict={lr.x:x, lr.y:y})\n",
    "        accuracy = (np.where(proba >= 0.5, 1, 0) == y).mean()\n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer()) #Make weights random again\n",
    "sess.run(tf.assign(lr.learning_rate, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    loss, proba, _ = sess.run([lr.loss, lr.proba, lr.train], feed_dict={lr.x:x, lr.y:y})\n",
    "    accuracy = (np.where(proba >= 0.5, 1, 0) == y).mean()\n",
    "    if i % 1000 == 0:\n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, batchsize, xsize, optimizer=tf.train.GradientDescentOptimizer):\n",
    "        self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "        self.x = tf.placeholder(shape=(batchsize, xsize), dtype=tf.float32, name=\"x\")\n",
    "        self.y = tf.placeholder(shape=(batchsize,1), dtype=tf.float32, name=\"y\")\n",
    "        \n",
    "        #Hidden Layer\n",
    "        w = tf.get_variable(\"w\", shape=(xsize, 2))\n",
    "        b = tf.get_variable(\"b\", shape=(2))\n",
    "        hidden = tf.sigmoid(tf.matmul(self.x,w) + b) \n",
    "        \n",
    "        w2 = tf.get_variable(\"w2\", shape=(2,1))\n",
    "        b2 = tf.get_variable(\"b2\", shape=(1))\n",
    "        logit = tf.matmul(hidden,w2) + b2\n",
    "        \n",
    "        self.proba = tf.sigmoid(logit)\n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=self.y)) \n",
    "        self.train = optimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"MLP\", reuse=None, initializer=initializer):\n",
    "    mlp = MLP(100, 2)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.assign(mlp.learning_rate, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    loss, proba,  _  = sess.run([mlp.loss, mlp.proba, mlp.train], feed_dict={mlp.x:x, mlp.y:y})\n",
    "    if i % 1000 == 0:\n",
    "        accuracy = (np.where(proba >= 0.5, 1, 0) == y).mean()                                                       \n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer()) #Make weights random again\n",
    "sess.run(tf.assign(mlp.learning_rate, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(20000):\n",
    "    loss, proba, _  = sess.run([mlp.loss, mlp.proba, mlp.train], feed_dict={mlp.x:x, mlp.y:y})\n",
    "    if i % 1000 == 0:\n",
    "        accuracy = (np.where(proba >= 0.5, 1, 0) == y).mean()                                                       \n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"MLP_adam\", reuse=None, initializer=initializer):\n",
    "    mlp = MLP(100, 2, optimizer=tf.train.AdamOptimizer)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.assign(mlp.learning_rate,0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    loss, proba,  _  = sess.run([mlp.loss, mlp.proba, mlp.train], feed_dict={mlp.x:x, mlp.y:y}) \n",
    "    if i % 1000 == 0:\n",
    "        accuracy = (np.where(proba >= 0.5, 1, 0) == y).mean()                                                       \n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer()) #Make weights random again\n",
    "sess.run(tf.assign(mlp.learning_rate, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    loss, proba,  _  = sess.run([mlp.loss, mlp.proba, mlp.train], feed_dict={mlp.x:x, mlp.y:y}) \n",
    "    if i % 1000 == 0:\n",
    "        accuracy = (np.where(proba >= 0.5, 1, 0) == y).mean()                                                       \n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Baseline optimizer: GradientDescent with starting learning rate of 0.1\n",
    "# Often the easiest optimizer out of the box: AdamOptimizer with starting learning rate of 0.001\n",
    "# To do quick checks of learning rate starting values try going up or down 10x (so check within 1.0, 0.1, 0.01)\n",
    "# Some have found Adam to learn faster, but SGD to eventually get better accuracy\n",
    "# Google state of the art Neural Machine Translation uses Adam for most of the training\n",
    "# and then finishes with some SGD (and then Reinforcement Learning!!)\n",
    "# https://arxiv.org/pdf/1609.08144v2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#scatter(x[:,0], x[:,1], c=y, cmap= ListedColormap(['#FF0000', '#0000FF']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class MLP3():\n",
    "    def __init__(self, batchsize, xsize, optimizer=tf.train.GradientDescentOptimizer):\n",
    "        self.learning_rate = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "        self.x = tf.placeholder(shape=(batchsize, xsize), dtype=tf.float32, name=\"x\")\n",
    "        self.y = tf.placeholder(shape=(batchsize,1), dtype=tf.float32, name=\"y\")\n",
    "        \n",
    "        #Hidden1\n",
    "        w = tf.get_variable(\"w\", shape=(xsize, 2))\n",
    "        b = tf.get_variable(\"b\", shape=(2))\n",
    "        hidden1 = tf.square(tf.matmul(self.x,w) + b)\n",
    "        \n",
    "        #hidden2\n",
    "        w2 = tf.get_variable(\"w2\", shape=(xsize, 2))\n",
    "        b2 = tf.get_variable(\"b2\", shape=(2))\n",
    "        hidden2 = tf.square(tf.matmul(hidden1,w2) + b2)\n",
    "        \n",
    "        w3 = tf.get_variable(\"w3\", shape=(2,1))\n",
    "        b3 = tf.get_variable(\"b3\", shape=(1))\n",
    "        logit = tf.matmul(hidden2,w3) + b3\n",
    "        \n",
    "        self.proba = tf.sigmoid(logit)\n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=self.y)) \n",
    "        self.train = optimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"MLP3\", reuse=False, initializer=tf.random_uniform_initializer(minval=-0.05, maxval=0.05)):\n",
    "    mlp = MLP3(None, 2, optimizer=tf.train.AdamOptimizer)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.assign(mlp.learning_rate, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.assign(mlp.learning_rate, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    loss, proba,  _  = sess.run([mlp.loss, mlp.proba, mlp.train], feed_dict={mlp.x:x, mlp.y:y}) \n",
    "    if i % 1000 == 0:\n",
    "        accuracy = (np.where(proba >= 0.5, 1, 0) == y).mean()                                                       \n",
    "        print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
